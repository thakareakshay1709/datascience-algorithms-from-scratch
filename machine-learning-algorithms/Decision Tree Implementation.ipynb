{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "992c7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "507d74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    Single tree node representation\n",
    "    \"\"\"\n",
    "    def __init__(self, gain=None, value=None, data_left=None, data_right=None, threshold=None, feature=None):\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "        self.data_left = data_left\n",
    "        self.data_right = data_right\n",
    "        self.threshold = threshold\n",
    "        self.feature = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2955648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Implementation of decision tree classifier algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, min_no_samples=5, max_depth_tree=2):\n",
    "        \"\"\"\n",
    "        Hyperparams initialzation\n",
    "        \"\"\"\n",
    "        self.min_no_samples = min_no_samples\n",
    "        self.max_depth_tree = max_depth_tree\n",
    "        self.root_node = None # initializing the top node of decision tree\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_entropy(s):\n",
    "        \"\"\"\n",
    "        Helper function to calculate entropy. Formula entropy = -summation(Pilog2(Pi)) i-> class label\n",
    "        \"\"\"\n",
    "        entropy = 0\n",
    "        count_class = np.bincount(np.array(s, np.int64)) # getting independent classes count and converting to int\n",
    "        percentages = count_class / len(s) # getting the percentage split of classes\n",
    "        for p in percentages:\n",
    "            if p > 0:\n",
    "                entropy += p * np.log2(p)\n",
    "        return - entropy\n",
    "    \n",
    "    def information_gain(self, parent, left_child, right_child):\n",
    "        \"\"\"\n",
    "        Helper function to calculate information gain of the split.\n",
    "        Formula : IG = E(parent) - (left_child/parent * E(left_child) + right_child/paretn * E(right_child))\n",
    "        \"\"\"\n",
    "        left = len(left_child) / len(parent)\n",
    "        right = len(right_child) / len(parent)\n",
    "        \n",
    "        info_gain = self.calculate_entropy(parent) - (left * self.calculate_entropy(left_child) + right * self.calculate_entropy(right_child))\n",
    "        return np.round(info_gain, 5)\n",
    "    \n",
    "    def get_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Function to calculate the best split\n",
    "        \"\"\"\n",
    "        best_split = {}\n",
    "        best_info_gain = -1\n",
    "        \n",
    "        n_rows, n_cols = X.shape # getting rows and columns\n",
    "        \n",
    "        for f_id in range(n_cols) : # get every feature\n",
    "            X_curr = X[:, f_id] # getting all data from every feature\n",
    "            for threshold in np.unique(X_curr): # get unique values of features\n",
    "                \n",
    "                # creating dataset\n",
    "                df = np.concatenate((X, y.reshape(1, -1).T), axis = 1)\n",
    "                df_left = np.array([row for row in df if row[f_id] <= threshold])\n",
    "                df_right = np.array([row for row in df if row[f_id] > threshold])\n",
    "                \n",
    "                # check if left and right splits have data\n",
    "                if len(df_left) > 0 and len(df_right) > 0:\n",
    "                    # getting values of target variables\n",
    "                    y_ = df[:, -1]\n",
    "                    y_left = df_left[:, -1]\n",
    "                    y_right = df_right[:, -1]\n",
    "                    \n",
    "                    gain = self.information_gain(y_, y_left, y_right)\n",
    "                    if gain > best_info_gain:\n",
    "                        # save the split\n",
    "                        best_split = {\n",
    "                            \"gain\" : gain,\n",
    "                            \"feature_idx\": f_id,\n",
    "                            \"threshold\": threshold,\n",
    "                            \"data_left\": df_left,\n",
    "                            \"data_right\": df_right,\n",
    "                        }\n",
    "                        # update gain\n",
    "                        best_info_gain = gain\n",
    "        return best_split\n",
    "    \n",
    "    def build_dec_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Building decision tree recursively\n",
    "        \"\"\"\n",
    "        # getting no of rows left\n",
    "        n_rows, _ = X.shape\n",
    "        \n",
    "        # checking exit conditions to break the build\n",
    "        if n_rows >= self.min_no_samples and depth <= self.max_depth_tree:\n",
    "            \n",
    "            # get the best split\n",
    "            best_split = self.get_best_split(X, y)\n",
    "\n",
    "            # check if split is not entirely pure\n",
    "            if best_split['gain'] > 0:\n",
    "                \n",
    "                left_side = self.build_dec_tree(X=best_split['data_left'][:,:-1], \n",
    "                                                y=best_split['data_left'][:,-1], depth=depth+1)\n",
    "                \n",
    "                right_side = self.build_dec_tree(X=best_split['data_right'][:,:-1], \n",
    "                                                 y=best_split['data_right'][:,-1], depth=depth+1)\n",
    "                \n",
    "                return TreeNode(gain=best_split['gain'], data_left=left_side, data_right=right_side, \n",
    "                                threshold=best_split['threshold'], feature=best_split['feature_idx'])\n",
    "        \n",
    "        # return value if split is pure\n",
    "        return TreeNode(value=Counter(y).most_common(1)[0][0])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Function to train decision tree classifier\n",
    "        \"\"\"\n",
    "        self.root = self.build_dec_tree(X, y)\n",
    "        \n",
    "    def set_predictions(self, x, tree):\n",
    "        \"\"\"\n",
    "        Calculate predictions for single observations\n",
    "        \"\"\"\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        feature_value = x[tree.feature]\n",
    "        if feature_value <= tree.threshold:\n",
    "            # traverse left\n",
    "            return self.set_predictions(x, tree.data_left)\n",
    "        if feature_value > tree.threshold:\n",
    "            # traverse right\n",
    "            return self.set_predictions(x, tree.data_right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Function to get predictions for every value of features\n",
    "        \"\"\"\n",
    "        return np.array([self.set_predictions(x, self.root) for x in X], np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71cc3a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origianal data : (150, 4), Train data : (120, 4)\n",
      "Decision Tree Classifiers result -> custom model: 0.9666666666666667, sklearn : 1.0\n"
     ]
    }
   ],
   "source": [
    "# loading play datasets from sklearn for multi class classification\n",
    "iris = load_iris()\n",
    "# getting training data and target data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(f\"Origianal data : {X.shape}, Train data : {X_train.shape}\")\n",
    "\n",
    "# custom model\n",
    "my_model = MyDecisionTreeClassifier()\n",
    "my_model.fit(X_train, y_train)\n",
    "preds = my_model.predict(X_test)\n",
    "my_score = accuracy_score(y_test, preds)\n",
    "\n",
    "# sk model\n",
    "\n",
    "sk_model = DecisionTreeClassifier()\n",
    "sk_model.fit(X_train, y_train)\n",
    "sk_preds = sk_model.predict(X_test)\n",
    "sk_score = accuracy_score(y_test, sk_preds)\n",
    "\n",
    "print(f\"Decision Tree Classifiers result -> custom model: {my_score}, sklearn : {sk_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262d80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
